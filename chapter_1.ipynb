{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: Your First Web Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter introduces web scraping by teaching you how to build a basic scraper using Python. You’ll learn about web page structures (HTML, CSS, and DOM), use Requests to fetch data, and Beautiful Soup to extract it. Along the way, you'll practice polite scraping and handle common challenges. By the end, you’ll have created your first functional scraper, gaining the skills to collect and organize data from websites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web scraping requires stripping away some of the shroud on interface—not just at the browser level (how it interprets all of this HTML, CSS, and JavaScript), but occasionally at the level of the network connection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A web browser can serve you with:\n",
    "\n",
    "- Creating packets of information for communicating with the OS \n",
    "- And interpreting these information in pretty format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**However this a web browser is a code and code can be splitted, rewritten or even reused**\n",
    "\n",
    "Which a python can make all of this in just a bunch of two or three lines of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<html>\\n<head>\\n<title>A Useful Page</title>\\n</head>\\n<body>\\n<h1>An Interesting Title</h1>\\n<div>\\nLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\\n</div>\\n</body>\\n</html>\\n'\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen # urllib is a standard Python library (meaning you don’t have to install anything extra to run this example)\n",
    "html = urlopen('http://pythonscraping.com/pages/page1.html')\n",
    "print(html.read())                      ## Same thing can be done with writing this in terminal (>python <code_file_name>.py) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command outputs the complete HTML code for page1 located at the URL *http://\n",
    "pythonscraping.com/pages/page1.html*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why we are taking in language of files and not pages?**\n",
    "\n",
    "Modern web pages are associated with another files (e.g.image files, JavaScript files or CSS files) \n",
    "\n",
    "And since you write in the HTML (img src=\"cuteKitten.jpg\"), The browser knows that he has to make a request to the server to get that data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Introduction to BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BeautifulSoup helps format and organize the messy web by fixing bad HTML and presenting us with easily traversable Python objects representing XML structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Python package manager PIP to install BS4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4         # This a IPython notebook trick you can use to run terminal commands in notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now after installation we can do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping Libraries Straight with Virtual Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Virtual environments in Python help isolate project dependencies, preventing conflicts and making management easier. By using them, you can organize your projects effectively, ensuring each one has its own set of libraries. This practice not only improves your workflow but also enhances proficiency, as it keeps projects clean and reduces the risk of dependency issues. The book offers specific instructions on how to set up and use a virtual environment, which you can reference when needed for more organized and efficient project development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head>\n",
      "<title>A Useful Page</title>\n",
      "</head>\n",
      "<body>\n",
      "<h1>An Interesting Title</h1>\n",
      "<div>\n",
      "Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n",
      "</div>\n",
      "</body>\n",
      "</html>\n",
      "\n",
      "<h1>An Interesting Title</h1>\n"
     ]
    }
   ],
   "source": [
    "html = urlopen('http://pythonscraping.com/pages/page1.html')\n",
    "\n",
    "bs = BeautifulSoup(html.read(), 'html.parser')\n",
    "# You can also write it like that\n",
    "# bs = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "print(bs)\n",
    "print(bs.h1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that bs object follows an amazing hierarchy And backing to that it can access any tag directly like h1, Actually you can write that in any prefered format (e.g. bs.html.body.h1, bs.html.h1 or even bs.body.h1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*You should know that (bs.h1) will bring only the first instance of the h1 tag which ideally should be the only one in the page but this not always true since web pages don't necessary follow these rules and a page may have more than one h1 you should be aware of that this will give you only the first instance of that tag*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BeautifulSoup object requires you with the type of parser to use which makes no differences in most cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install lxml  -> Another parser used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html>\n",
       "<head>\n",
       "<title>A Useful Page</title>\n",
       "</head>\n",
       "<body>\n",
       "<h1>An Interesting Title</h1>\n",
       "<div>\n",
       "Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n",
       "</div>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lxml\n",
    "\n",
    "html = urlopen('http://pythonscraping.com/pages/page1.html')\n",
    "bs = BeautifulSoup(html.read(), 'lxml')\n",
    "\n",
    "bs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of lxml parser advantages over html.parser is its forgiving, additional features (e.g. fixing unclosed and misnested tags) and some-what faster than html.parser, However it has an annoying disadvantage which is that it has to be installed separately and depends\n",
    "on third-party C libraries to function. This can cause problems for portability and ease of use, compared to html.parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install html5lib     -> A third parser we can use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you may face a problem of that it does not recognize the parser so i have checked that every installation is done properly and finally i have restarted the kernel on notebook and it worked, You try that if you faced a problem like that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html><head>\n",
       "<title>A Useful Page</title>\n",
       "</head>\n",
       "<body>\n",
       "<h1>An Interesting Title</h1>\n",
       "<div>\n",
       "Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n",
       "</div>\n",
       "\n",
       "\n",
       "</body></html>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import html5lib\n",
    "\n",
    "html = urlopen('http://pythonscraping.com/pages/page1.html')\n",
    "bs = BeautifulSoup(html.read(), 'html5lib')\n",
    "\n",
    "bs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting Reliably and Handling Exceptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping is a the process that may take the whole day running and you may sleep leaving your scraper running and here where a problem may came and your scraper hits an unexpected data format and stop running so handling exceptions in the first place is important as much as the process itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backing to the past code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = urlopen('http://www.pythonscraping.com/pages/page1.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two main things can go wrong in this line:\n",
    "\n",
    "- An HTTP error will be returned. In all cases, the urlopen function will throw the generic exception HTTPError\n",
    "- If the server is not found at all, urlopen will throw an URLError. This indicates that no server could be reached at all, and, because the remote server is responsible for returning HTTP status codes, an HTTPError cannot be thrown, and the more serious URLError must be caught."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL caught!\n"
     ]
    }
   ],
   "source": [
    "from urllib.error import HTTPError\n",
    "from urllib.error import URLError\n",
    "\n",
    "try:\n",
    "    html = urlopen('http://www.pythonscraping.com/pages/page1.html')\n",
    "except HTTPError as HTTPerr:\n",
    "    print(HTTPerr)\n",
    "except URLError as URLerr:\n",
    "    print(URLerr)\n",
    "else:\n",
    "    pass                        # Only if there is no exception\n",
    "finally:\n",
    "    print(\"URL caught!\")        # Always excute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every time you access a tag in a BeautifulSoup object, it’s smart to add a check to make sure the tag actually exists. If you attempt to access a tag that does not exist, BeautifulSoup will return a None object. The problem is, attempting to access a tag on a None object itself will result in an AttributeError being thrown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(bs.<a_name_of_a_tag_that_does_not_exist>)       -> Which will throw a None type object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The real problem comes when you try ti access an attribute or another tag inside that tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(bs.<a_tag_that_doesn't_exist>.<another_tag>)        -> This will pass an AttributeError that None type has no attribute call anther_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag was not found\n"
     ]
    }
   ],
   "source": [
    "try: unkown_content = bs.does_not_exist_tag.another_tag\n",
    "except AttributeError as attrerr: print(\"Tag was not found\")\n",
    "else:\n",
    "    if unkown_content == None: print(\"Tag was not found\")\n",
    "    else: print(unkown_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, Well formatted code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>An Interesting Title</h1>\n"
     ]
    }
   ],
   "source": [
    "def getTitle(url):\n",
    "    try:\n",
    "        html = urlopen(url)  \n",
    "    except HTTPError as e: \n",
    "        return None\n",
    "    \n",
    "    try: \n",
    "        bs = BeautifulSoup(html.read(), 'html.parser')\n",
    "        title = bs.body.h1\n",
    "    except AttributeError as e:\n",
    "        return None\n",
    "    \n",
    "    return title\n",
    "\n",
    "title = getTitle('http://www.pythonscraping.com/pages/page1.html')\n",
    "\n",
    "if title == None:\n",
    "    print('Title could not be found')\n",
    "else:\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note That"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When writing scrapers, it’s important to think about the overall pattern of your code in order to handle exceptions and make it readable at the same time. You’ll also likely want to heavily reuse code. Having generic functions such as getSiteHTML and getTitle (complete with thorough exception handling) makes it easy to quickly— and reliably—scrape the web.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter covered the foundational aspects of web scraping with Python, focusing on using Requests to retrieve web page content and Beautiful Soup to parse and extract data. It also highlighted the importance of virtual environments for managing project dependencies, ensuring an organized and conflict-free development process. The chapter provided the essential tools and knowledge needed to begin scraping data and set the stage for exploring more advanced techniques in subsequent chapters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
